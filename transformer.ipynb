{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0l8NgEdwYkDpLTs/j35T0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabeorosan/papers-explained/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math"
      ],
      "metadata": {
        "id": "8o4HochZB95Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Load data\n",
        "train_iter, valid_iter, test_iter = WikiText2()\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<pad>', '<bos>', '<eos>'])\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "\n",
        "# Prepare data\n",
        "def data_process(raw_text_iter):\n",
        "    data = [torch.tensor([vocab[token] for token in tokenizer(item)], dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.nn.utils.rnn.pad_sequence(data, padding_value=vocab[\"<pad>\"])\n",
        "\n",
        "train_data = data_process(train_iter)\n",
        "valid_data = data_process(valid_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BOS_IDX = vocab[\"<bos>\"]\n",
        "EOS_IDX = vocab[\"<eos>\"]\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    src = torch.cat([torch.cat([torch.tensor([BOS_IDX]), item, torch.tensor([EOS_IDX])]) for item in data_batch]).view(len(data_batch), -1)\n",
        "    tgt = torch.cat([item for item in data_batch]).view(len(data_batch), -1)\n",
        "    return src, tgt\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qd1JcULLA8MV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
        "        self.fc_out = nn.Linear(d_model, ntoken)\n",
        "        self.positional_encoding = self.generate_positional_encoding(d_model, max_seq_length)\n",
        "\n",
        "    def generate_positional_encoding(self, d_model, max_len):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        return pe\n",
        "    def forward(self, src, tgt):\n",
        "        print('hi mom')\n",
        "        # ... [rest of the forward method]\n"
      ],
      "metadata": {
        "id": "1pBQyzgXCo8m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given vocab from earlier\n",
        "ntoken = len(vocab)\n",
        "d_model = 32  # Reduced embedding dimension\n",
        "nhead = 2  # Reduced number of heads in multihead attention\n",
        "num_encoder_layers = 1  # Just one encoder layer\n",
        "num_decoder_layers = 1  # Just one decoder layer\n",
        "dim_feedforward = 64  # Reduced FFN size\n",
        "max_seq_length = 100  # Limit sequence length\n",
        "\n",
        "model = TransformerModel(ntoken, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n"
      ],
      "metadata": {
        "id": "HOGogMe4C9om"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDz1Cbo_DGOK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}